{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary study for experiment with WordNet as thesaurus\n",
    "\n",
    "Use wordnet distance between phrases to provide neighbours.\n",
    "\n",
    "### Firstly check how many of the NPs in my corpora occur in WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from discoutils.tokens import DocumentFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = set()\n",
    "for s in wn.all_synsets():\n",
    "    for lemma in s.lemmas():\n",
    "        if lemma.name().count('_') == 1:\n",
    "            phrases.add(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylist = list(phrases)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist [0].synset().path_similarity(mylist[1].synset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../thesisgenerator/features_in_labelled/all_features.txt') as inf:\n",
    "    my_phrases = set(map(str.strip, inf.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isabel',\n",
       " 'eisenstein_portrayal',\n",
       " 'parrot_figure_thing',\n",
       " 'water_don',\n",
       " 'guantanamera_cover_territory',\n",
       " 'package_furry',\n",
       " 'chase_hummie',\n",
       " 'youhot',\n",
       " 'prefect_condition',\n",
       " 'eastern_variety']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_phrases = []\n",
    "for p in list(my_phrases):\n",
    "    p = DocumentFeature.from_string(p)\n",
    "    f = '_'.join(t.text for t in p.tokens)\n",
    "    formatted_phrases.append(f)\n",
    "formatted_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genus_boletus',\n",
       " 'artificial_joint',\n",
       " 'take_after',\n",
       " 'deficit_spending',\n",
       " 'merlangus_merlangus',\n",
       " 'climbing_frame',\n",
       " 'free-reed_instrument',\n",
       " 'cerebrospinal_meningitis',\n",
       " 'ringworm_shrub',\n",
       " 'genus_mayaca']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_phrases = [x.name().lower() for x in phrases]\n",
    "wn_phrases[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared = set(wn_phrases).intersection(set(formatted_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['occipital_lobe',\n",
       " 'taste_sensation',\n",
       " 'discount_rate',\n",
       " 'taxi_dancer',\n",
       " 'left_hemisphere',\n",
       " 'occupational_therapy',\n",
       " 'broom_closet',\n",
       " 'social_worker',\n",
       " 'shasta_daisy',\n",
       " 'change_course']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(shared)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now make a thesaurus out of these using path similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmata = [x for x in phrases if x.name().lower() in shared]\n",
    "len(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lemmata_index = {b:a for a,b in enumerate(lemmata)}\n",
    "sims = np.zeros((len(lemmata), len(lemmata)))\n",
    "\n",
    "for i, (lemma1, lemma2) in enumerate(combinations(lemmata, 2)):\n",
    "    p1, p2 = lemmata_index[lemma1], lemmata_index[lemma2]\n",
    "    sim = lemma1.synset().path_similarity(lemma2.synset())\n",
    "    sims[p1, p2] = sim\n",
    "    sims[p2, p1] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([             nan,   7.58196100e-06,   4.33254914e-06, ...,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (2, 3)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(combinations([1,2,3], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
